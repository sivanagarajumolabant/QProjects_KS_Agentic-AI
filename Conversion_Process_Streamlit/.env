# Azure OpenAI Configuration
LLM_PROVIDER=azure_openai
# LLM_PROVIDER=gemini
# LLM_PROVIDER=anthropic


OUTPUT_PATH=output


AZURE_OPENAI_ENDPOINT=https://ai-testgeneration707727059630.openai.azure.com/
AZURE_OPENAI_API_KEY=wBjgqz2HegyKwtsNCInM8T0aGAYsSFQ2sPHrv2N9BNhmmreKVJ1NJQQJ99BDACYeBjFXJ3w3AAAAACOGQOtm
AZURE_DEPLOYMENT_NAME=gpt4-deployment
AZURE_OAI_MODEL=gpt-4o
AZURE_OPENAI_API_VERSION=2024-12-01-preview
AZURE_OPENAI_TEMPERATURE=0.7
AZURE_OPENAI_MAX_TOKENS=4096

# OpenAI Configuration
OPENAI_API_KEY=your-openai-api-key
OPENAI_MODEL=gpt-4o
OPENAI_TEMPERATURE=0.7
OPENAI_MAX_TOKENS=4096

# Anthropic Configuration
ANTHROPIC_API_KEY=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
ANTHROPIC_MODEL=claude-3-5-sonnet-20241022
ANTHROPIC_TEMPERATURE=0.7
ANTHROPIC_MAX_TOKENS=4096

# Groq Configuration
GROQ_API_KEY=your-groq-api-key
GROQ_MODEL=llama3-70b-8192
GROQ_TEMPERATURE=0.7
GROQ_MAX_TOKENS=4096

# Google Gemini Configuration
GEMINI_API_KEY=AIzaSyCeBF3bPaNGaFNzJgLIpOsSeqG8S2FRmNk
GEMINI_MODEL=gemini-2.0-flash
GEMINI_TEMPERATURE=0.7
GEMINI_MAX_TOKENS=4096

# Ollama Configuration (for local models)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3
OLLAMA_TEMPERATURE=0.7
OLLAMA_MAX_TOKENS=4096
