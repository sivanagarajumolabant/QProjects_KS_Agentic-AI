{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4ea641e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from typing import TypedDict, Optional\n",
    "import pandas as pd\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain.prompts import PromptTemplate\n",
    "from hashlib import md5\n",
    "from IPython.display import Image, display\n",
    "\n",
    "AZURE_OPENAI_CONFIG = {\n",
    "    \"deployment_name\": \"gpt4-deployment\",\n",
    "    \"embedding_deployment\": \"text-embedding-3-large\",\n",
    "    \"api_key\": \"wBjgqz2HegyKwtsNCInM8T0aGAYsSFQ2sPHrv2N9BNhmmreKVJ1NJQQJ99BDACYeBjFXJ3w3AAAAACOGQOtm\",\n",
    "    \"api_base\": \"https://ai-testgeneration707727059630.openai.azure.com/\",\n",
    "    \"api_version\": \"2024-12-01-preview\"\n",
    "}\n",
    "\n",
    "DATA_FOLDER = 'C:/QProjects/TestData_AI/New_data'\n",
    "DDL_FOLDER = 'C:/QProjects/TestData_AI/ddls'\n",
    "OUTPUT_FOLDER = \"generated_test_data_new_data_test\"\n",
    "NUM_RECORDS = 10\n",
    "\n",
    "# ================================\n",
    "# STATE MODEL\n",
    "# ================================\n",
    "\n",
    "class GenerationState(TypedDict):\n",
    "    tables: Optional[dict[str, pd.DataFrame]]\n",
    "    ddls: Optional[dict[str, str]]\n",
    "    foreign_keys: Optional[dict[str, list]]\n",
    "    vectorstores: Optional[dict]\n",
    "    generated: Optional[dict]\n",
    "\n",
    "# ================================\n",
    "# INITIALIZE AZURE OPENAI MODELS\n",
    "# ================================\n",
    "llm = AzureChatOpenAI(\n",
    "    deployment_name=AZURE_OPENAI_CONFIG[\"deployment_name\"],\n",
    "    azure_endpoint=AZURE_OPENAI_CONFIG[\"api_base\"],  # <-- updated\n",
    "    api_key=AZURE_OPENAI_CONFIG[\"api_key\"],\n",
    "    api_version=AZURE_OPENAI_CONFIG[\"api_version\"],\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "embedding_model = AzureOpenAIEmbeddings(\n",
    "    deployment=AZURE_OPENAI_CONFIG[\"embedding_deployment\"],\n",
    "    azure_endpoint=AZURE_OPENAI_CONFIG[\"api_base\"],  # <-- updated\n",
    "    api_key=AZURE_OPENAI_CONFIG[\"api_key\"],\n",
    "    api_version=AZURE_OPENAI_CONFIG[\"api_version\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a7fe9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ================================\n",
    "# LANGGRAPH AGENT NODES\n",
    "# ================================\n",
    "def load_csvs_node(state: GenerationState) -> GenerationState:\n",
    "    print(\"Running load_csvs_node...\")\n",
    "\n",
    "    tables = {}\n",
    "    if not os.path.exists(DATA_FOLDER):\n",
    "        raise ValueError(f\"DATA_FOLDER does not exist: {DATA_FOLDER}\")\n",
    "    \n",
    "    files = os.listdir(DATA_FOLDER)\n",
    "    print(\"Files in CSV folder:\", files)\n",
    "\n",
    "    for file in files:\n",
    "        if file.endswith(\".csv\"):\n",
    "            table_name = file.replace(\".csv\", \"\")\n",
    "            path = os.path.join(DATA_FOLDER, file)\n",
    "            df = pd.read_csv(path)\n",
    "            print(f\"Loaded {file} with shape {df.shape}\")\n",
    "            if not df.empty:\n",
    "                tables[table_name] = df\n",
    "\n",
    "    if not tables:\n",
    "        raise ValueError(\"No non-empty CSV files found to load.\")\n",
    "\n",
    "    print(\"Returning updated state with tables:\", list(tables.keys()))\n",
    "    print(\"load_csvs_node=======\")\n",
    "    return {**state, \"tables\": tables}\n",
    "\n",
    "def load_ddls_node(state: GenerationState) -> GenerationState:\n",
    "    ddls = {}\n",
    "    for file in os.listdir(DDL_FOLDER):\n",
    "        if file.endswith(\".sql\"):\n",
    "            table_name = file.replace(\".sql\", \"\")\n",
    "            with open(os.path.join(DDL_FOLDER, file), \"r\") as f:\n",
    "                ddls[table_name] = f.read()\n",
    "    print(\"load_ddls_node=======\")\n",
    "    return {**state, \"ddls\": ddls}\n",
    "\n",
    "# def infer_foreign_keys_node(state: GenerationState) -> GenerationState:\n",
    "#     tables = state.tables\n",
    "#     fk_map = {}\n",
    "#     primary_keys = {}\n",
    "#     for table, df in tables.items():\n",
    "#         for col in df.columns:\n",
    "#             if col.endswith(\"_id\"):\n",
    "#                 primary_keys.setdefault(col, []).append(table)\n",
    "\n",
    "#     for table, df in tables.items():\n",
    "#         fk_map[table] = {}\n",
    "#         for col in df.columns:\n",
    "#             if col in primary_keys and table not in primary_keys[col]:\n",
    "#                 fk_map[table][col] = df[col].dropna().astype(str).unique().tolist()\n",
    "#     return {**state, \"foreign_keys\": fk_map}\n",
    "\n",
    "import re\n",
    "\n",
    "def extract_keys_from_ddl(ddl: str):\n",
    "    pk_pattern = r\"PRIMARY KEY\\s*\\(([^)]+)\\)\"\n",
    "    fk_pattern = r\"FOREIGN KEY\\s*\\(([^)]+)\\)\\s*REFERENCES\\s+(\\S+)\\(([^)]+)\\)\"\n",
    "\n",
    "    primary_keys = []\n",
    "    foreign_keys = {}\n",
    "\n",
    "    for match in re.finditer(pk_pattern, ddl, re.IGNORECASE):\n",
    "        columns = [col.strip() for col in match.group(1).split(\",\")]\n",
    "        primary_keys.extend(columns)\n",
    "\n",
    "    for match in re.finditer(fk_pattern, ddl, re.IGNORECASE):\n",
    "        fk_column = match.group(1).strip()\n",
    "        ref_table = match.group(2).strip().split('.')[-1]  # remove schema if present\n",
    "        ref_column = match.group(3).strip()\n",
    "        foreign_keys[fk_column] = {\"ref_table\": ref_table, \"ref_column\": ref_column}\n",
    "    print(primary_keys, \"primamry keys ===\")\n",
    "    print(foreign_keys, \"foreign_keys ===\")\n",
    "\n",
    "    return primary_keys, foreign_keys\n",
    "\n",
    "def infer_foreign_keys_node(state: GenerationState) -> GenerationState:\n",
    "    print(\"Running infer_foreign_keys_node...\")\n",
    "    tables = state['tables']\n",
    "    ddls = state[\"ddls\"]\n",
    "    fk_map = {}\n",
    "    for table_name, df in tables.items():\n",
    "        fk_map[table_name] = {}\n",
    "        ddl = ddls.get(table_name, \"\")\n",
    "        _, fk_defs = extract_keys_from_ddl(ddl)\n",
    "\n",
    "        for fk_col, ref_info in fk_defs.items():\n",
    "            ref_table = ref_info[\"ref_table\"]\n",
    "            print(ref_table, \"ref table==========\")\n",
    "            print(tables.keys(),\"tablekeys===========\")\n",
    "            if ref_table in tables:\n",
    "                existing_values = tables[ref_table][ref_info[\"ref_column\"]].dropna().astype(str).unique().tolist()\n",
    "                fk_map[table_name][fk_col] = existing_values\n",
    "\n",
    "    # print(\"Inferred foreign keys:\")\n",
    "    for k, v in fk_map.items():\n",
    "        print(f\"{k}: {list(v.keys())}\")\n",
    "    print(\"Foreign keys : \", fk_map)\n",
    "    return {**state, \"foreign_keys\": fk_map}\n",
    "\n",
    "\n",
    "def build_vectorstores_node(state: GenerationState) -> GenerationState:\n",
    "    print('==============build_vectorstores_node')\n",
    "    vectorstores = {}\n",
    "    for table, df in state['tables'].items():\n",
    "        docs = df.astype(str).apply(lambda row: \", \".join(row), axis=1).tolist()\n",
    "        vectorstores[table] = FAISS.from_texts(docs, embedding_model)\n",
    "    return {**state, \"vectorstores\": vectorstores}\n",
    "\n",
    "def find_parent_table(fk_col: str, fk_map: dict[str, dict[str, list]]) -> str:\n",
    "    print('==============find_parent_table')\n",
    "    for table, mappings in fk_map.items():\n",
    "        if fk_col in mappings:\n",
    "            return table\n",
    "    return None\n",
    "\n",
    "def generate_test_data_node(state: GenerationState) -> GenerationState:\n",
    "    print('==============enerate_test_data_node')\n",
    "    def is_duplicate(new_row, existing_rows):\n",
    "        new_hash = md5(json.dumps(new_row, sort_keys=True).encode()).hexdigest()\n",
    "        return new_hash in existing_rows\n",
    "\n",
    "    generated_data = {}\n",
    "    tables = list(state[\"tables\"].keys())\n",
    "    fk_map = state[\"foreign_keys\"]\n",
    "\n",
    "    prompt_template = PromptTemplate.from_template(\"\"\"\n",
    "You are a test data generator. Generate ONE realistic, non-duplicate row for the table `{table_name}`.\n",
    "\n",
    "DDL Definition:\n",
    "{ddl}\n",
    "\n",
    "Sample Data Context:\n",
    "{examples}\n",
    "\n",
    "Foreign Key Constraints (if any):\n",
    "{fk_values}\n",
    "\n",
    "Guidelines:\n",
    "- Follow the DDL strictly (types, nullability, constraints)\n",
    "- Use realistic names, emails, products, descriptions, prices, timestamps, etc.\n",
    "- Avoid duplicates (no exact same rows)\n",
    "- Sometimes leave nullable fields blank\n",
    "- Respect relationships and existing FK values\n",
    "- Ensure unique constraints (like emails, phone numbers, user_ids) are followed\n",
    "\n",
    "Return ONLY a valid JSON object, without extra commentary or markdown.\n",
    "\"\"\")\n",
    "\n",
    "    for table in tables[:1]:\n",
    "        df = state[\"tables\"][table]\n",
    "        table_rows = []\n",
    "        existing_hashes = set()\n",
    "\n",
    "        for _ in range(NUM_RECORDS):\n",
    "            retries = 5\n",
    "            for _ in range(retries):\n",
    "                sample_contexts = state[\"vectorstores\"][table].similarity_search(\"generate\", k=3)\n",
    "                context = \"\\n\".join([doc.page_content for doc in sample_contexts])\n",
    "\n",
    "                fk_values = fk_map.get(table, {}).copy()\n",
    "                for fk_col in fk_values:\n",
    "                    parent_table = find_parent_table(fk_col, fk_map)\n",
    "                    if parent_table in generated_data:\n",
    "                        parent_values = generated_data[parent_table][fk_col].dropna().unique().tolist()\n",
    "                        if parent_values:\n",
    "                            fk_values[fk_col] = parent_values\n",
    "\n",
    "                ddl_text = state[\"ddls\"].get(table, \"\")\n",
    "                prompt = prompt_template.format(\n",
    "                    table_name=table,\n",
    "                    ddl=ddl_text,\n",
    "                    examples=context,\n",
    "                    fk_values=json.dumps(fk_values)\n",
    "                )\n",
    "\n",
    "                response = llm([HumanMessage(content=prompt)])\n",
    "                try:\n",
    "                    new_row = json.loads(response.content.strip())\n",
    "                    if not is_duplicate(new_row, existing_hashes):\n",
    "                        existing_hashes.add(md5(json.dumps(new_row, sort_keys=True).encode()).hexdigest())\n",
    "                        table_rows.append(new_row)\n",
    "                        break\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "\n",
    "        generated_data[table] = pd.DataFrame(table_rows)\n",
    "    return {**state, \"generated\": generated_data}\n",
    "    \n",
    "\n",
    "def save_outputs_node(state: GenerationState) -> GenerationState:\n",
    "    os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "    print(\"Saving Generated Data to CSV files...\")\n",
    "    for table, df in state[\"generated\"].items():\n",
    "        output_path = os.path.join(OUTPUT_FOLDER, f\"{table}_generated.csv\")\n",
    "        df.to_csv(output_path, index=False)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ef1dc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# BUILD LANGGRAPH FLOW\n",
    "# ================================\n",
    "workflow = StateGraph(GenerationState)\n",
    "\n",
    "workflow.add_node(\"load_csvs\", load_csvs_node)\n",
    "workflow.add_node(\"load_ddls\", load_ddls_node)\n",
    "workflow.add_node(\"infer_foreign_keys\", infer_foreign_keys_node)\n",
    "workflow.add_node(\"build_vectorstores\", build_vectorstores_node)\n",
    "workflow.add_node(\"generate_data\", generate_test_data_node)\n",
    "workflow.add_node(\"save_data\", save_outputs_node)\n",
    "\n",
    "\n",
    "workflow.set_entry_point(\"load_csvs\")\n",
    "workflow.add_edge(\"load_csvs\", \"load_ddls\")\n",
    "workflow.add_edge(\"load_csvs\", \"load_ddls\")\n",
    "workflow.add_edge(\"load_ddls\", \"infer_foreign_keys\")\n",
    "workflow.add_edge(\"infer_foreign_keys\", \"build_vectorstores\")\n",
    "workflow.add_edge(\"build_vectorstores\", \"generate_data\")\n",
    "workflow.add_edge(\"generate_data\", \"save_data\")\n",
    "workflow.add_edge(\"save_data\", END)\n",
    "\n",
    "# ================================\n",
    "# RUN WORKFLOW\n",
    "# ================================\n",
    "flow = workflow.compile()\n",
    "# display(Image(flow.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "313b22e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING...\n",
      "Running load_csvs_node...\n",
      "Files in CSV folder: ['1-registration.registrationmaster.csv', '2-registration.patient.csv', '3-registration.addressmaster.csv', '4-registration.patientvisadetail.csv', '5-billing.patientpolicymaster.csv', '6-billing.opmaster.csv', '7-billing.servicerequest.csv', '8-billing.servicerequestdetails.csv', '9-billing.generalaccountmaster_bkp.csv']\n",
      "Loaded 1-registration.registrationmaster.csv with shape (6, 6)\n",
      "Loaded 2-registration.patient.csv with shape (6, 134)\n",
      "Loaded 3-registration.addressmaster.csv with shape (6, 29)\n",
      "Loaded 4-registration.patientvisadetail.csv with shape (6, 18)\n",
      "Loaded 5-billing.patientpolicymaster.csv with shape (6, 8)\n",
      "Loaded 6-billing.opmaster.csv with shape (6, 24)\n",
      "Loaded 7-billing.servicerequest.csv with shape (6, 28)\n",
      "Loaded 8-billing.servicerequestdetails.csv with shape (6, 52)\n",
      "Loaded 9-billing.generalaccountmaster_bkp.csv with shape (6, 21)\n",
      "Returning updated state with tables: ['1-registration.registrationmaster', '2-registration.patient', '3-registration.addressmaster', '4-registration.patientvisadetail', '5-billing.patientpolicymaster', '6-billing.opmaster', '7-billing.servicerequest', '8-billing.servicerequestdetails', '9-billing.generalaccountmaster_bkp']\n",
      "load_csvs_node=======\n",
      "load_ddls_node=======\n",
      "Running infer_foreign_keys_node...\n",
      "['registrationid'] primamry keys ===\n",
      "{} foreign_keys ===\n",
      "['registrationid'] primamry keys ===\n",
      "{} foreign_keys ===\n",
      "[] primamry keys ===\n",
      "{'addresstypeid': {'ref_table': 'addresstypemaster', 'ref_column': 'addresstypeid'}} foreign_keys ===\n",
      "addresstypemaster ref table==========\n",
      "dict_keys(['1-registration.registrationmaster', '2-registration.patient', '3-registration.addressmaster', '4-registration.patientvisadetail', '5-billing.patientpolicymaster', '6-billing.opmaster', '7-billing.servicerequest', '8-billing.servicerequestdetails', '9-billing.generalaccountmaster_bkp']) tablekeys===========\n",
      "['registrationid'] primamry keys ===\n",
      "{} foreign_keys ===\n",
      "['patientpolicymasterid'] primamry keys ===\n",
      "{} foreign_keys ===\n",
      "[] primamry keys ===\n",
      "{} foreign_keys ===\n",
      "['requestid', 'locationid'] primamry keys ===\n",
      "{} foreign_keys ===\n",
      "[] primamry keys ===\n",
      "{} foreign_keys ===\n",
      "['accountno'] primamry keys ===\n",
      "{} foreign_keys ===\n",
      "1-registration.registrationmaster: []\n",
      "2-registration.patient: []\n",
      "3-registration.addressmaster: []\n",
      "4-registration.patientvisadetail: []\n",
      "5-billing.patientpolicymaster: []\n",
      "6-billing.opmaster: []\n",
      "7-billing.servicerequest: []\n",
      "8-billing.servicerequestdetails: []\n",
      "9-billing.generalaccountmaster_bkp: []\n",
      "Foreign keys :  {'1-registration.registrationmaster': {}, '2-registration.patient': {}, '3-registration.addressmaster': {}, '4-registration.patientvisadetail': {}, '5-billing.patientpolicymaster': {}, '6-billing.opmaster': {}, '7-billing.servicerequest': {}, '8-billing.servicerequestdetails': {}, '9-billing.generalaccountmaster_bkp': {}}\n",
      "==============build_vectorstores_node\n",
      "==============enerate_test_data_node\n",
      "Saving Generated Data to CSV files...\n"
     ]
    }
   ],
   "source": [
    "print(\"STARTING...\")\n",
    "initial_state = GenerationState(\n",
    "    tables=None,\n",
    "    ddls=None,\n",
    "    foreign_keys=None,\n",
    "    vectorstores=None,\n",
    "    generated=None\n",
    ")\n",
    "\n",
    "final_state = flow.invoke(initial_state)\n",
    "# final_state = flow.invoke(GenerationState())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
